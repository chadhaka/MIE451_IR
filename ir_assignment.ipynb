{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "* Put all your imports, and path constants in the next cells\n",
    "* Make sure ***MATERIALS_DIR*** points to the directory where you extracted the Zip file.\n",
    "* Make sure all your paths are **relative to ** ***MATERIALS_DIR*** and **NOT hard-coded** in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# Put all your imports here\n",
    "\n",
    "from whoosh import index, writing\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import *\n",
    "from whoosh.qparser import QueryParser\n",
    "import os, os.path\n",
    "import shutil\n",
    "# NLTK Library\n",
    "import nltk\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATERIALS_DIR = r\"C:\\DSS_Fall2017_Assign2\"\n",
    "#\n",
    "# Put other path constants here\n",
    "#\n",
    "DOCUMENTS_DIR = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\government\\documents\")\n",
    "INDEX_DIR = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\government\\index1\")\n",
    "QUER_FILE = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\government\\topics\\gov.topics\")\n",
    "QRELS_FILE = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\government\\qrels\\gov.qrels\")\n",
    "OUTPUT_FILE = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\government\\myres\")\n",
    "TREC_EVAL = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\trec_eval\\trec_eval.exe\")\n",
    "INDEX_DIR2 = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\government\\index2\")\n",
    "OUTPUT_FILE2 = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\government\\myres2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trec_eval [-h] [-q] [-m measure[.params] [-c] [-n] [-l <num>]\n",
      "   [-D debug_level] [-N <num>] [-M <num>] [-R rel_format] [-T results_format]\n",
      "   rel_info_file  results_file \n",
      " \n",
      "Calculate and print various evaluation measures, evaluating the results  \n",
      "in results_file against the relevance info in rel_info_file. \n",
      " \n",
      "There are a fair number of options, of which only the lower case options are \n",
      "normally ever used.   \n",
      " --help:\n",
      " -h: Print full help message and exit. Full help message will include\n",
      "     descriptions for any measures designated by a '-m' parameter, and\n",
      "     input file format descriptions for any rel_info_format given by '-R'\n",
      "     and any top results_format given by '-T.'\n",
      "     Thus to see all info about preference measures use\n",
      "          trec_eval -h -m all_prefs -R prefs -T trec_results \n",
      " --version:\n",
      " -v: Print version of trec_eval and exit.\n",
      " --query_eval_wanted:\n",
      " -q: In addition to summary evaluation, give evaluation for each query/topic\n",
      " --measure measure_name[.measure_params]:\n",
      " -m measure: Add 'measure' to the lists of measures to calculate and print.\n",
      "    If 'measure' contains a '.', then the name of the measure is everything\n",
      "    preceeding the period, and everything to the right of the period is\n",
      "    assumed to be a list of parameters for the measure, separated by ','. \n",
      "    There can be multiple occurrences of the -m flag.\n",
      "    'measure' can also be a nickname for a set of measures. Current \n",
      "    nicknames include \n",
      "       'official': the main measures often used by TREC\n",
      "       'all_trec': all measures calculated with the standard TREC\n",
      "                   results and rel_info format files.\n",
      "       'set': subset of all_trec that calculates unranked values.\n",
      "       'prefs': Measures not in all_trec that calculate preference measures.\n",
      " --complete_rel_info_wanted:\n",
      " -c: Average over the complete set of queries in the relevance judgements  \n",
      "     instead of the queries in the intersection of relevance judgements \n",
      "     and results.  Missing queries will contribute a value of 0 to all \n",
      "     evaluation measures (which may or may not be reasonable for a  \n",
      "     particular evaluation measure, but is reasonable for standard TREC \n",
      "     measures.) Default is off.\n",
      " --level_for_rel num:\n",
      " -l<num>: Num indicates the minimum relevance judgement value needed for \n",
      "      a document to be called relevant. Used if rel_info_file contains \n",
      "      relevance judged on a multi-relevance scale.  Default is 1. \n",
      " --nosummary:\n",
      " -n: No summary evaluation will be printed\n",
      " --Debug_level num:\n",
      " -D <num>: Debug level.  1 and 2 used for measures, 3 and 4 for merging\n",
      "     rel_info and results, 5 and 6 for input.  Currently, num can be of the\n",
      "     form <num>.<qid> and only qid will be evaluated with debug info printed.\n",
      "     Default is 0.\n",
      " --Number_docs_in_coll num:\n",
      " -N <num>: Number of docs in collection Default is MAX_LONG \n",
      " -Max_retrieved_per_topic num:\n",
      " -M <num>: Max number of docs per topic to use in evaluation (discard rest). \n",
      "      Default is MAX_LONG.\n",
      " --Judged_docs_only:\n",
      " -J: Calculate all values only over the judged (either relevant or  \n",
      "     nonrelevant) documents.  All unjudged documents are removed from the \n",
      "     retrieved set before any calculations (possibly leaving an empty set). \n",
      "     DO NOT USE, unless you really know what you're doing - very easy to get \n",
      "     reasonable looking numbers in a file that you will later forget were \n",
      "     calculated  with the -J flag.  \n",
      " --Rel_info_format format:\n",
      " -R format: The rel_info file is assumed to be in format 'format'.  Current\n",
      "    values for 'format' include 'qrels', 'prefs', 'qrels_prefs'.  Note not\n",
      "    all measures can be calculated with all formats.\n",
      " --Results_format format:\n",
      " -T format: the top results_file is assumed to be in format 'format'. Current\n",
      "    values for 'format' include 'trec_results'. Note not all measures can be\n",
      "    calculated with all formats.\n",
      " --Zscore Zmean_file:\n",
      " -Z Zmean_file: Instead of printing the raw score for each measure, print\n",
      "    a Z score instead. The score printed will be the deviation from the mean\n",
      "    of the raw score, expressed in standard deviations, where the mean and\n",
      "    standard deviation for each measure and query are found in Zmean_file.\n",
      "    If mean is not in Zmeanfile for a measure and query, -1000000 is printed.\n",
      "    Zmean_file format is ascii lines of form \n",
      "       qid  measure_name  mean  std_dev\n",
      " \n",
      " \n",
      "Standard evaluation procedure:\n",
      "For each of the standard TREC measures requested, a ranked list of\n",
      "of relevance judgements is created corresponding to each ranked retrieved doc,\n",
      "A rel judgement is set to -1 if the document was not in the pool (not in \n",
      "rel_info_file) or -2 if the document was in the pool but unjudged (some \n",
      "measures (infAP) allow the pool to be sampled instead of judged fully).  \n",
      "Otherwise it is set to the value in rel_info_file. \n",
      "Most measures, but not all, will treat -1 or -2 the same as 0, \n",
      "namely nonrelevant.  Note that relevance_level is used to \n",
      "determine if the document is relevant during score calculations. \n",
      "Queries for which there is no relevance information are ignored. \n",
      "Warning: queries for which there are relevant docs but no retrieved docs \n",
      "are also ignored by default.  This allows systems to evaluate over subsets  \n",
      "of the relevant docs, but means if a system improperly retrieves no docs,  \n",
      "it will not be detected.  Use the -c flag to avoid this behavior. \n",
      "\n",
      "-----------------------\n",
      "Results_file format: Standard 'trec_results'\n",
      "Lines of results_file are of the form \n",
      "     030  Q0  ZF08-175-870  0   4238   prise1 \n",
      "     qid iter   docno      rank  sim   run_id \n",
      "giving TREC document numbers (a string) retrieved by query qid  \n",
      "(a string) with similarity sim (a float).  The other fields are ignored, \n",
      "with the exception that the run_id field of the last line is kept and \n",
      "output.  In particular, note that the rank field is ignored here; \n",
      "internally ranks are assigned by sorting by the sim field with ties  \n",
      "broken deterministicly (using docno). \n",
      "Sim is assumed to be higher for the docs to be retrieved first. \n",
      "File may contain no NULL characters. \n",
      "Lines may contain fields after the run_id; they are ignored. \n",
      "\n",
      "-----------------------\n",
      "Rel_info_file format: Standard 'qrels'\n",
      "Relevance for each docno to qid is determined from rel_info_file, which \n",
      "consists of text tuples of the form \n",
      "   qid  iter  docno  rel \n",
      "giving TREC document numbers (docno, a string) and their relevance (rel,  \n",
      "a non-negative integer less than 128, or -1 (unjudged)) \n",
      "to query qid (a string).  iter string field is ignored.   \n",
      "Fields are separated by whitespace, string fields can contain no whitespace. \n",
      "File may contain no NULL characters. \n",
      "\n",
      "-----------------------\n",
      "Individual measure documentation for requested measures\n",
      "runid\n",
      "    Runid given by results input file.\n",
      "num_q\n",
      "    Number of topics results averaged over.  May be different from\n",
      "    number of topics in the results file if -c was used on the command line \n",
      "    in which case number of topics in the rel_info file is used.\n",
      "num_ret\n",
      "    Number of documents retrieved for topic. \n",
      "    May be affected by Judged_docs_only and Max_retrieved_per_topic command\n",
      "    line parameters (as are most measures).\n",
      "    Summary figure is sum of individual topics, not average.\n",
      "num_rel\n",
      "    Number of relevant documents for topic. \n",
      "    May be affected by Judged_docs_only and Max_retrieved_per_topic command\n",
      "    line parameters (as are most measures).\n",
      "    Summary figure is sum of individual topics, not average.\n",
      "num_rel_ret\n",
      "    Number of relevant documents retrieved for topic. \n",
      "    May be affected by Judged_docs_only and Max_retrieved_per_topic command\n",
      "    line parameters (as are most measures).\n",
      "    Summary figure is sum of individual topics, not average.\n",
      "map\n",
      "    Mean Average Precision\n",
      "    Precision measured after each relevant doc is retrieved, then averaged\n",
      "    for the topic, and then averaged over topics (if more than one).\n",
      "    This is the main single-valued number used to compare the entire rankings\n",
      "    of two or more retrieval methods.  It has proven in practice to be useful\n",
      "    and robust.\n",
      "    The name of the measure is unfortunately inaccurate since it is \n",
      "    calculated for a single topic (and thus don't want both 'mean' and\n",
      "    'average') but was dictated by common usage and the need to distiguish\n",
      "    map from Precision averaged over topics (I had to give up my attempts to\n",
      "    call it something else!)\n",
      "    History: Developed by Chris Buckley after TREC 1.\n",
      "    Cite: 'Retrieval System Evaluation', Chris Buckley and Ellen Voorhees.\n",
      "    Chapter 3 in TREC: Experiment and Evaluation in Information Retrieval\n",
      "    edited by Ellen Voorhees and Donna Harman.  MIT Press 2005\n",
      "gm_map\n",
      "    Geometric Mean Average Precision\n",
      "    This is the same measure as 'map' (see description of 'map') on an\n",
      "    individual topic, but the geometric mean is calculated when averaging\n",
      "    over topics.  This rewards methods that are more consistent over topics\n",
      "    as opposed to methods which do very well for some topics but very poorly\n",
      "    for others.\n",
      "    gm_ap is reported only in the summary over all topics, not for individual\n",
      "    topics.\n",
      "Rprec\n",
      "    Precision after R documents have been retrieved.\n",
      "    R is the total number of relevant docs for the topic.  \n",
      "    This is a good single point measure for an entire retrieval\n",
      "    ranking that averages well since each topic is being averaged\n",
      "    at an equivalent point in its result ranking.\n",
      "    Note that this is the point that Precision = Recall.\n",
      "    History: Originally developed for IR rankings by Chris Buckley\n",
      "    after TREC 1, but analogs were used in other disciplines previously.\n",
      "    (the point where P = R is an important one!)\n",
      "    Cite: 'Retrieval System Evaluation', Chris Buckley and Ellen Voorhees.\n",
      "    Chapter 3 in TREC: Experiment and Evaluation in Information Retrieval\n",
      "    edited by Ellen Voorhees and Donna Harman.  MIT Press 2005\n",
      "bpref\n",
      "    Main binary preference measure.\n",
      "    Fraction of the top R nonrelevant docs that are retrieved after each\n",
      "    relevant doc. Put another way: when looking at the R relevant docs, and\n",
      "    the top R nonrelevant docs, if all relevant docs are to be preferred to\n",
      "    nonrelevant docs, bpref is the fraction of the preferences that the\n",
      "    ranking preserves.\n",
      "    Cite: 'Retrieval Evaluation with Incomplete Information', Chris Buckley\n",
      "    and Ellen Voorhees. In Proceedings of 27th SIGIR, 2004.\n",
      "recip_rank\n",
      "    Reciprocal Rank of the first relevant retrieved doc.\n",
      "    Measure is most useful for tasks in which there is only one relevant\n",
      "    doc, or the user only wants one relevant doc.\n",
      "iprec_at_recall\n",
      "    Interpolated Precision at recall cutoffs.\n",
      "    This is the data shown in the standard Recall-Precision graph.\n",
      "    The standard cutoffs and interpolation are needed to average data over\n",
      "    multiple topics; otherwise, how is a topic with 5 relevant docs averaged\n",
      "    with a topic with 3 relevant docs for graphing purposes?  The Precision \n",
      "    interpolation used here is\n",
      "      Int_Prec (rankX) == MAX (Prec (rankY)) for all Y >= X.\n",
      "    Default usage: -m iprec_at_recall.0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1 ...\n",
      "P\n",
      "    Precision at cutoffs\n",
      "    Precision measured at various doc level cutoffs in the ranking.\n",
      "    If the cutoff is larger than the number of docs retrieved, then\n",
      "    it is assumed nonrelevant docs fill in the rest.  Eg, if a method\n",
      "    retrieves 15 docs of which 4 are relevant, then P20 is 0.2 (4/20).\n",
      "    Precision is a very nice user oriented measure, and a good comparison\n",
      "    number for a single topic, but it does not average well. For example,\n",
      "    P20 has very different expected characteristics if there 300\n",
      "    total relevant docs for a topic as opposed to 10.\n",
      "    Note:   trec_eval -m P.50 ...\n",
      "    is different from \n",
      "            trec_eval -M 50 -m set_P ...\n",
      "    in that the latter will not fill in with nonrel docs if less than 50\n",
      "    docs retrieved\n",
      "    Cutoffs must be positive without duplicates\n",
      "    Default param: -m P.5,10,15,20,30,100,200,500,1000\n"
     ]
    }
   ],
   "source": [
    "!$TREC_EVAL -h -m official\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Provide your text answers in the following two markdown cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (a): Provide answer to Q1 (a) here [markdown cell]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trec_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e69f65244ccd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrec_eval\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'trec_eval' is not defined"
     ]
    }
   ],
   "source": [
    "trec_eval [-h] [-m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (b): Provide answer to Q1 (b) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (a): Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code for creating the index here (you can add more cells).\n",
    "# Make sure you save the final index in the variable INDEX_Q2, your query parser in QP_Q2, and your searcher in SEARCHER_Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INDEX_Q2 = None # Replace None with your index for Q2\n",
    "QP_Q2 = None # Replace None with your query parser for Q2\n",
    "SEARCHER_Q2 = None # Replace None with your searcher for Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (b): Provide answer to Q2 (b) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (c): Provide answer to Q2(c) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (a): Provide answer to Q3 (a) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (b): Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code for creating the index here (you can add more cells).\n",
    "# Make sure you save the final index in the variable INDEX_Q3, your query parser in QP_Q3, and your searcher in SEARCHER_Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INDEX_Q3 = None # Replace None with your index for Q3\n",
    "QP_Q3 = None # Replace None with your query parser for Q3\n",
    "SEARCHER_Q3 = None # Replace None with your searcher for Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (c): Provide answer to Q3 (c) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (d): Provide answer to Q3 (d) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (e): Provide answer to Q3 (e) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (f): Provide answer to Q3 (f) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (Graduate Students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRAD_STUDENT = False # change to True if you are a grad student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (a): Provide answer to Q4 (a) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (b): Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code for creating the index here (you can add more cells).\n",
    "# Make sure you save the final index in the variable INDEX_Q4, your query parser in QP_Q4, and your searcher in SEARCHER_Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INDEX_Q4 = None # Replace None with your index for Q4\n",
    "QP_Q4 = None # Replace None with your query parser for Q4\n",
    "SEARCHER_Q4 = None # Replace None with your searcher for Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (c): Provide answer to Q4 (a) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (d): Provide answer to Q4 (a) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (e): Provide answer to Q4 (a) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (f): Provide answer to Q4 (a) here [markdown cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the following cells to make sure your code returns the correct value types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.index import FileIndex\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh.searching import Searcher\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert \"MATERIALS_DIR\" in globals(), \"variable MATERIALS_DIR does not exists\"\n",
    "assert(os.path.isdir(os.path.join(MATERIALS_DIR))), \"MATERIALS_DIR folder does not exists\"\n",
    "assert(os.path.isdir(os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\"))), \"invalid folder structure\"\n",
    "assert(os.path.isdir(os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\government\\documents\"))), \"invalid folder structure\"\n",
    "print(\"Paths validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(isinstance(INDEX_Q2, FileIndex)), \"Index Type\"\n",
    "assert(isinstance(QP_Q2, QueryParser)), \"Query Parser Type\"\n",
    "assert(isinstance(SEARCHER_Q2, Searcher)), \"Searcher Type\"\n",
    "print(\"Q2 Types Validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(isinstance(INDEX_Q3, FileIndex)), \"Index Type\"\n",
    "assert(isinstance(QP_Q3, QueryParser)), \"Query Parser Type\"\n",
    "assert(isinstance(SEARCHER_Q3, Searcher)), \"Searcher Type\"\n",
    "print(\"Q3 Types Validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Validation (Graduate Students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert((not GRAD_STUDENT) or isinstance(INDEX_Q4, FileIndex)), \"Index Type\"\n",
    "assert((not GRAD_STUDENT) or isinstance(QP_Q4, QueryParser)), \"Query Parser Type\"\n",
    "assert((not GRAD_STUDENT) or isinstance(SEARCHER_Q4, Searcher)), \"Searcher Type\"\n",
    "print(\"Q4 Types Validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
